\part{Regression and optimization}
\section{Introduction}
\subsection*{}
\begin{frame}[label=contents_opt]
  \frametitle{Today's outline}
  \mode<beamer>{
    \only<1>{\tableofcontents}
  }
  \only<2>{\tableofcontents[currentsection,currentsubsection]}
\end{frame}

\frame{
  \frametitle{Overview}
  \vfill
  \begin{itemize}
  \item We are going to fit measurements to models today
  \item You will also learn what $R^2$ actually means
  \item We get introduced to constrained and unconstrained optimization.
  \item We will use the simplex method to solve linear programming problems (LP)
  \end{itemize}
  \vfill
}

\section{Curve fitting}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}[fragile,label={slidedatacreate}]{Let's do an 'experiment' to gather data}
    \begin{lstlisting}
def generate_random_data(N=101,p=[1,1/3,1.5,3.5],draw=False):
    # Generate linear space of control points
    # N -  Number of data points
    # p -  Coefficients of polynomial
    x = np.linspace(0, 1, N)  # Points (independent variable)

    # Generate 'measurement values' with errors following a normal distribution
    # Initialize randomizer
    pd = norm(loc=0, scale=0.1)
    # Add scatter data to the polynomial
    y = np.polyval(p,x) + pd.rvs(size=N)

    # Plot the generated data
    if draw:
        plt.plot(x, y, 'x')
        plt.show()      
    return x,y   
    \end{lstlisting}
    Gather some data by calling the function and storing \lstinline|x| and \lstinline|y|
\end{frame}

\frame{
  \frametitle{Fitting models to data}
  \vfill
  \centering
  \input{tikzplots/scatter1.tikz}
  \vfill
}

\begin{frame}[fragile]
  \frametitle{How to fit a model to the data?}
  We would like to fit the following model to the data:
  \[
    \hat{y} = a_0 x^3 + a_1 x^2 + a_2x + a_3
  \]
  \vfill \pause
  First attempt - using the \lstinline|polyfit| function we have seen with the interpolation lecture:
  \begin{lstlisting}
def fit_using_polyfit(x,y,n=2,draw=False):
    p = np.polyfit(x,y,n)
    if draw:
        plt.plot(x, y, 'x')
        plt.plot(x, np.polyval(p,x), '-')
        plt.show()     
    return p
  \end{lstlisting}
  If we print \lstinline|p|, we get the coefficients. But this is a black box, what does it do?
\end{frame}

\frame{
  \frametitle{How to fit a model to the data?}
  We would like to fit the following model to the data:
  \[
    \hat{y} = a_0 x^3 + a_1 x^2 + a_2x + a_3
  \]
  \vfill \pause
  Attempt to solve a linear system: If we have $N$ data points, we could write the model as the product of a matrix and a vector:
  \begin{columns}\
    \column{0.6\textwidth}
      \[
      \begin{bmatrix}
      \hat{y}_0 \\
      \hat{y}_1 \\
      \hat{y}_2 \\
      \vdots \\
      \hat{y}_{N-1}
    \end{bmatrix}
    =
    \left. \begin{bmatrix}
      x_0^3   &   x_0^2  &x_0       & 1     \\
      x_1^3   &   x_1^2  &x_1       & 1     \\
      x_2^3   &   x_2^2  &x_2       & 1     \\
      \vdots  &   \vdots &\vdots   & \vdots \\
      x_{N-1}^3   &   x_{N-1}^2  &x_{N-1}       & 1         \end{bmatrix}    
    \begin{bmatrix}
      a_0 \\
      a_1 \\
      a_2 \\
      a_3
    \end{bmatrix} \right.
      \]
    \column{0.4\textwidth}
    \tikz{\node[emphblock,text width=\columnwidth]{
    \begin{center}
    $
      \hat{y} = Xa
    $
    \end{center}
    $X$ is called the design matrix and $a$ are the fit parameters.
    }}
  \end{columns}
  \vfill
}

\frame{
  \frametitle{Residuals}
  Second step: work out the residuals for each data point:
  \[
    d_i = \left(y_i - \hat{y}_i\right)
  \]
  \pause
  Third step: work out the sum of squares of the residuals:
  \[
    \text{SSE} = \sum_i d_i^2 = \sum_i  \left(y_i - \hat{y}_i\right)^2
  \]
  This can be written using the dot-product operation:
    \[
    \text{SSE} = \sum_i d_i^2 = d \cdot d = d^T \cdot d = \left(y_i - \hat{y}_i\right)^T \cdot \left(y_i - \hat{y}_i\right)
  \]
}


\frame{
  \frametitle{Minimizing the sum of squares}
  Choose the parameter vector such that the sum of squares of the residuals is minimized; the partial derivative with respect to each parameter should be set to zero:
  \[
    \frac{\partial}{\partial a_i} \left[ \left(y - \left(Xa\right)^T\right)\left(y - Xa\right) \right]
  \]
  \pause
  In Python, we can solve our linear system $\hat{Y} = Xa$ simply by running \lstinline$a = np.linalg.solve(X,y)$.
  \pause
  \begin{itemize}
    \item If there are more data points ($N>4$), we can write an analogue, but maybe a consistent solution does not exist (the system is over specified).
    \item However, Python will find values for the vector a which minimize $\norm{y-aX}^2$ , so i.e. a least squares fit.
  \end{itemize}
}

\frame{
  \frametitle{Example: fitting a linear model using least-squares}
  Assume the model is given as:
  \[
  \hat{y} = ax + b
  \]
  Then the error is computed as the sum of squares:
  \[
    E(a,b) = \sum_{i=0}^{n-1} \left(y_i - \hat{y}_i\right)^2 = \sum_{i=0}^{n-1} \left(y_i - (ax_i + b)\right)^2 
  \]
  \pause
  The partial derivatives with respect to $a$ and $b$ are:
  \[
    \frac{\partial E}{\partial a} = -2 \sum_{i=0}^{n-1} x_i \left(y_i - (ax_i + b)\right)
  \]
  \[
    \frac{\partial E}{\partial b} = -2 \sum_{i=0}^{n-1} \left(y_i - (ax_i + b)\right)
  \]
}

{\nologo
\frame{
  \frametitle{Example: fitting a linear model using least-squares}
  Setting the partial derivatives to zero gives the following system of equations:
  \[
    \begin{bmatrix}
    \sum_i x_i^2 & \sum_i x_i \\
    \sum_i x_i & N
    \end{bmatrix}
    \begin{bmatrix}
    a \\
    b
    \end{bmatrix}
    =
    \begin{bmatrix}
    \sum_i x_i y_i \\
    \sum_i y_i
    \end{bmatrix}
  \]
  \pause
  Exercise: make a program that takes $x$ and $y$ as input and returns the coefficients $a$ and $b$.
  \begin{itemize}
    \item Either set up and solve the linear system
    \item Or substitute equations/use Cramer's rule to obtain:
    \[
    a = \frac{n\sum_i x_i y_i - \sum_i x_i \sum_i y_i}{n\sum_i x_i^2 - (\sum_i x_i)^2}, \, b = \frac{\sum_i y_i - a\sum_i x_i}{n}
    \]
  \end{itemize}
  \pause
  \begin{itemize}
    \item Try for yourself to derive the equations for a second degree polynomial model
    \item Create a program that implements a generic polynomial model of degree $n$ using:
    \[
    \begin{bmatrix}
    \sum_i x_i^{2n} & \sum_i x_i^{2n-1} & \cdots & \sum_i x_i^n \\
    \sum_i x_i^{2n-1} & \sum_i x_i^{2n-2} & \cdots & \sum_i x_i^{n-1} \\
    \vdots & \vdots & \ddots & \vdots \\
    \sum_i x_i & \sum_i x_i & \cdots & N
    \end{bmatrix}
    \begin{bmatrix}
    a_n \\
    a_{n-1} \\
    \vdots \\
    a_0
    \end{bmatrix}
    =
    \begin{bmatrix}
    \sum_i x_i^n y_i \\
    \sum_i x_i^{n-1} y_i \\
    \vdots \\
    \sum_i y_i
    \end{bmatrix}
  \]
  \end{itemize}
}
}

\begin{frame}[fragile]{Example solution}
    \begin{lstlisting}
def fit_linear_model(x,y,draw=False):
    assert x.shape == y.shape, 'x and y must have the same shape'
    n = x.shape[0]
    sx2 = np.sum(x**2)
    sx = np.sum(x)
    sxy = np.sum(x*y)
    sy = np.sum(y)

    A = np.array([[sx2,sx],[sx,n]])
    b = np.array([sxy,sy])
    a,b = np.linalg.solve(A,b)
    # Alternatively, we could use the following:
    # a = (n*sxy - sx*sy)/(n*sx2 - sx**2)
    # b = (sy - a*sx)/n
    print(f'Found linear model: f(x) = {a:1.4}x + {b:1.4}')
    
    if draw:
        plt.plot(x,y,'x')
        plt.plot(x,a*x+b,'-')
        plt.title('Fit using linear model')
        plt.show()
    return a,b
    \end{lstlisting}
\end{frame}


% \frame{ 
%   \frametitle{Using Matlab's linear solver for fitting}
%   \begin{itemize}
%     \item If there are more data points ($N>4$), we can write an analogue, but maybe a consistent solution does not exist (the system is over specified).
%     \item However, matlab will find values for the vector a which minimize $\norm{y-aX}^2$ , so i.e. a least squares fit.
%   \end{itemize}
% }

\begin{frame}[fragile] 
  \frametitle{Fitting our problem: Least squares solver}
  As a follow-up of the script provided in slide~\ref{slidedatacreate}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \begin{lstlisting}
def fit_using_lstsqr(x,y,n=2,draw=False):
    xmat = np.vander(x,n+1)
    sol = np.linalg.lstsq(xmat,y,rcond=None)
    A = sol[0]
    yhat = xmat@A
    if draw:
        plt.plot(x,y,'x')
        plt.plot(x,yhat,'-')
        plt.title('Fit using lstsq')
        plt.show()
    return A,yhat
    \end{lstlisting}
    \column{0.5\textwidth}
      \input{tikzplots/scatter-model.tikz}
  \end{columns}
\end{frame}


% \begin{frame}[fragile] 
%   \frametitle{Fitting our problem: Excel}
%     \begin{itemize}
%       \item Create a column with the independent and dependent data series ($x$ and $y$)
%       \item Create a column that computes $\hat{y}$, keeping the coefficients as separate cells
%       \item Compute the sum of squares of the residuals (another column, sum the results)
%       \item Use the solver to minimize this sum, modifying the coefficient cells
%       \item Note: regression in Excel + display equation is dangerous if you choose the 'line' plot (use scatter if you can)
%     \end{itemize}
% \end{frame}

\begin{frame}[fragile] 
  \frametitle{How good is the model?}
  \input{tikzplots/residuals.tikz}

  \begin{itemize}
    \item For a model to make sense the data points should be scattered randomly around the model predictions, the mean of the residuals $d$ should be zero: $d_i = \left( y_i - \hat{y}_i\right)$
    \item Itâ€™s always good to check if the residuals are not correlated with the measured values, if that is the case, it can indicate that your model is wrong.
  \end{itemize}
\end{frame}

\section{Regression}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}[fragile] 
  \frametitle{Regression coefficients}
    \begin{itemize}
    \item Variance measured in the data (y) is:
    \[
      \sigma^2_y = \frac{1}{N} \sum_i \left (y_i - \overline{y}\right)^2
    \]
    \item Variance of the residuals is:
    \[
      \sigma^2_\text{error} = \frac{1}{N} \sum_i \left (d_i\right)^2
    \]
        \item Variance in the model is:
    \[
      \sigma^2_\text{model} = \frac{1}{N} \sum_i \left (\hat{y}_i - \overline{\hat{y}}\right)^2
    \]
  \end{itemize}
\end{frame}

\begin{frame}[fragile] 
  \frametitle{Regression coefficients}
  Given that the error is uncorrelated we can state that:
    \[
      \sigma^2_y = \sigma^2_\text{error}+\sigma^2_\text{model}
    \]
    \[
      R^2 = \frac{\sigma^2_\text{model}}{\sigma^2_y} = 1 - \frac{\sigma^2_\text{error}}{\sigma^2_y}
    \]
    \[
      R^2 = 1 - \frac{\text{SSE}}{\text{SST}}
    \]
    \begin{itemize}
      \item SSE: Sum of errors (residuals) squared (difference between data and model)
      \item SST: Total sum of squares (variance of the data)
      \item SSR: Sum of squares (model)
   \end{itemize}
\end{frame}

% \begin{frame}[fragile] 
%   \frametitle{Regression coefficients}
%     \begin{itemize}
%       \item An uncorrellated error ($\overline{d} \rightarrow 0$) means that SSE, SST and SSR will have $\chi^2$ -distributions and the ratios will have an F-distribution. If SSR/SSE is large, the model is good!
%       \item There is a chance that the model is rubbish, but that SSR/SSE will yield a good value, Analysis of Variance (ANOVA) will be a good tool to calculate the probability of such a thing happening!
%    \end{itemize}
% \end{frame}

\begin{frame}[fragile] 
  \frametitle{Back to the example}
  \begin{columns}
    \column{0.4\textwidth}
      \begin{center}
        The statistics: \\ \vskip1em
        % \input{tikzplots/table.tex}
        \begin{tabular}{lc}
          & Value \\ 
         \hline$N$   & 100 \\ 
         SSE   & 32.042 \\ 
         SST   & 896.907 \\ 
         SSR   & 928.950 \\ 
         $R^2$ & 0.964 \\ 
         \hline\end{tabular}
      \end{center}
    \column{0.6\textwidth}
      \input{tikzplots/scatter-model.tikz}
  \end{columns}
\end{frame}

\section{Fitting numerical models}
\subsection*{}
\againframe<2>{contents_opt}

{\nologo
\begin{frame}[fragile]
  \frametitle{Curve fitting from command line: scipy.optimize.curve\_fit}
  Python offers various non-linear parameter and curve fitting tools that can be run from the command line. The function \lstinline$curve_fit$ from the \lstinline$scipy.optimize$ module allows to fit a model to a given dataset. Again, based on the data generated in slide~\ref{slidedatacreate}:
    \begin{lstlisting}
from poly_regression import generate_random_data
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt
import numpy as np

# Define the model function
def curve_fit_model_1(xdata, a0, a1, a2, a3):
    return a0*xdata**3 + a1*xdata**2 + a2*xdata + a3
    \end{lstlisting}
    \begin{lstlisting}
if __name__ == '__main__':
    x,y = generate_random_data()
    a0 = [1, 2, 1, 3] # Initial guess of coefficients
    
    # Perform fitting, store resulting coeffs in a_fit
    a_fit, _ = curve_fit(curve_fit_model_1, x, y, p0=a0)
    
    # Run the model once more, with fitted coefficients
    y_model = curve_fit_model_1(x, *a_fit) # unpack coefficients using *
    \end{lstlisting}
    % The model function takes individual parameters separately, we use the unpacking syntax (\lstinline$*$) to pass the fitted parameters when generating the \lstinline$y_model$ data.
\end{frame}
}

\begin{frame}[fragile]
  \frametitle{Curve fitting from command line: scipy.optimize.curve\_fit}
  For fitting a polynomial model, this function works as well as \lstinline|polyfit|. But it also allows other (much more complex) types of model to be defined:
    \begin{lstlisting}
def curve_fit_model_2(xdata, a0, a1, a2):
    return a0*np.exp(a1*xdata) + a2

# Initial guess of coefficients
a0 = [1,1,1]

# Perform fitting of the data to another model
a_fit, _ = curve_fit(curve_fit_model_2, x, y, p0=a0)

# Run the model once more, with fitted coefficients
y_model = curve_fit_model_2(x, *a_fit)
    \end{lstlisting}
    The model functions take individual parameters separately, we use the unpacking syntax (\lstinline$*$) to pass the fitted parameters when generating the \lstinline$y_model$ data.
\end{frame}

\begin{frame}[fragile] 
  \frametitle{Dynamic fitting of non-linear equations}
  You may encounter situations where the model data is slightly more complicated to obtain (e.g., a numerical model based on ODEs where coefficients are unknown), or you want to perform fitting of multiple functions/coefficients, or just want to automate things via scripts. Python's Scipy library gives access to powerful functions such as \lstinline$least_squares$ and \lstinline$curve_fit$.
\end{frame}
  
\begin{frame}[fragile] 
  \frametitle{General use of scipy.optimize.least\_squares}
  \begin{lstlisting}
from scipy.optimize import least_squares

result = least_squares(fun, k0, bounds=(lb, ub), xtol=1.0E-6, max_nfev=1000)
  \end{lstlisting}
  \begin{itemize}
    \item \lstinline$fun$ is a function handle to the fit criterion (e.g., \lstinline$myFitCrit$). The fit criterion function \lstinline$myFitCrit$ should return the residuals vector, e.g., $d_i = \left(y_i - \hat{y}_i\right)$. Here, $y_i$ would again be the measurement data and $\hat{y}$ the solution computed by a model.
    \item \lstinline$k0$ is the initial guess for the fitting coefficient (or: array of initial guesses when fitting multiple coefficients).
    \item \lstinline$lb$ and \lstinline$ub$ are the lower and upper boundaries for \lstinline$k0$. These should both be the size of the \lstinline$k0$-array.
    \item Use arguments such as \lstinline$xtol$ and \lstinline$max_nfev$ for more fine-grained control on the fit procedure. 
  \end{itemize}  
\end{frame}
  
  
  \begin{frame}[fragile] 
  \frametitle{General use of scipy.optimize.curve\_fit}
  \begin{lstlisting}
from scipy.optimize import curve_fit

popt, pcov = curve_fit(fun, xdata, ydata, p0=k0, bounds=(lb, ub))
  \end{lstlisting}
  \begin{itemize}
    \item \lstinline$fun$ is the model function that you want to fit to your data. It takes the independent variable as the first argument and the parameters to fit as separate remaining arguments.
    \item \lstinline$xdata$ and \lstinline$ydata$ are the data points that you are fitting the model function to.
    \item \lstinline$k0$ is the initial guess for the parameters to be fitted.
    \item \lstinline$lb$ and \lstinline$ub$ are the lower and upper bounds for the parameters, respectively.
    \item \lstinline$popt$ will contain the optimized parameters, and \lstinline$pcov$ will contain the covariance matrix, which can give you an idea of the uncertainties of the estimates.
  \end{itemize}  
  \end{frame}
    

  \begin{frame}[fragile] 
    \frametitle{Example use of scipy.optimize.curve\_fit}
    We have experimental data stored in a file, possibly in a .csv or .txt format, containing $T$ and $U$ data. We want to fit a model with coefficients $k_1$ and $k_2$ with the following structure:
    \[
      \frac{du}{dt} = -k_1 u + k_2
    \]
    \pause
    \begin{itemize}
      \item First, we define a function that describes our model:
      \begin{lstlisting}
import scipy as sp
import numpy as np

def simpleode(t, u, k1, k2):
    dudt = -k1*u + k2
    return dudt
      \end{lstlisting}
      Note that we supply the coefficients \(k_1\) and \(k_2\) as arguments to the function.
      \pause
      \item We create a fit criterion function:
      \begin{lstlisting}
def fitcrit(t,k1, k2):
    u0 = [1.0]
    tspan = [0,max(t)]
    sol = sp.integrate.solve_ivp(simpleode, tspan, u0, args=(k1, k2), t_eval=t)
    return sol.y[0]
      \end{lstlisting}
    \end{itemize}
    \end{frame}
    
    \begin{frame}[fragile] 
    \frametitle{Example use of scipy.optimize.curve\_fit}
    Now let's make a script that uses \lstinline$curve_fit$ to yield k-values fitted to our dataset:
    \pause
    \begin{lstlisting}
# Load your data here (adjust as necessary)
T, U = np.loadtxt('./scripts/optimization/tudataset1.txt',unpack=True, skiprows=1)

# Initial guesses for model parameters
k0 = [1.0, 1.0]

# Perform the curve fitting
params, params_covariance = sp.optimize.curve_fit(fitcrit, T, U, p0=k0)
print('Fitted coefficients:', params)
    \end{lstlisting}
    Our fitted coefficients are stored in \lstinline|params|. The \lstinline|params_covariance| gives an estimate of the covariance of the estimated parameters, offering an insight into the uncertainty of the fit.
    \end{frame}      

\begin{frame}[fragile] 
  \frametitle{Example fitted ODE model}
  \centering\input{tikzplots/lsqnonlinfit.tikz}
\end{frame}


\begin{frame}[fragile,label={postprocess_fit}] 
  \frametitle{Postprocessing of results}
  The data returned by \lstinline|curve_fit| or \lstinline|least_squares| can be used to obtain the 95\% confidence intervals for the fitted parameters. Recall we stored/computed the parameters and covariance matrix:
  \begin{lstlisting}[language=Python]
params, params_covariance = curve_fit(fitcrit, T, U, p0=k0)
  \end{lstlisting}
  We can use the square root of the diagonal of the covariance matrix, multiplied by a factor from the t-distribution to get the confidence bounds:
  \begin{lstlisting}
from scipy import stats
import numpy as np

alpha = 0.05 # 95% confidence interval = 100*(1-alpha)
n = len(U)    # number of data points
p = len(params) # number of parameters

dof = max(0, n - p) # number of degrees of freedom
# t value for the dof and confidence level
tval = stats.t.ppf(1.0-alpha/2., dof) 
sigma = np.sqrt(np.diag(params_covariance))
ci = sigma * tval

print('Confidence intervals:')
print('k1:', params[0] - ci[0], params[0] + ci[0])
print('k2:', params[1] - ci[1], params[1] + ci[1])
  \end{lstlisting}
  \end{frame}

\begin{frame}[fragile] 
    \frametitle{Similar case, but using scipy.optimize.least\_squares}
    \begin{itemize}
      \item First, we define a function that describes our model:
      \begin{lstlisting}
def simpleode(t, u, k1, k2):
    dudt = -k1*u + k2
    return dudt
      \end{lstlisting}
      \pause
      \item Create a fit criterion function that computes the residuals between the model and the data:
      \begin{lstlisting}
def fitcrit(k,expdata):
    t = expdata[0]
    u = expdata[1]
    u0 = [u[0]]
    tspan = [0,max(t)]
    k1,k2 = k
    sol = sp.integrate.solve_ivp(simpleode, tspan, u0, args=(k1, k2), t_eval=t)
    return sol.y[0] - u
      \end{lstlisting}
      Note: the function \lstinline|fitcrit| takes the parameters to fit as a single argument, so we need to unpack them inside the function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile] 
    \frametitle{Similar case, but using scipy.optimize.least\_squares}
    \begin{itemize}
      \item Perform the fitting:
      \begin{lstlisting}
# Initial guesses for model parameters
k0 = [1.0, 1.0]

# Perform the curve fitting
expdata = [T,U]
fit = sp.optimize.least_squares(fitcrit, k0, args=(expdata,))
params = fit.x
params_covariance = fit.jac
print('Fitted coefficients:', params)
      \end{lstlisting}
      \item Postprocessing of results is similar to the \lstinline|curve_fit| case.
    \end{itemize}
\end{frame}

\againframe{postprocess_fit}

%   \begin{frame}[fragile]
%     \frametitle{Second example: curve\_fit}
%     The model doesn't have to be an ODE; it can be any model that is part of the fitting goal function. Let's consider an (adapted) version of the Laplace equation solver, where we can set a single, central node to a certain temperature. We will illustrate this using Python, where we define a function to solve the Laplace equation:
    
%     \begin{lstlisting}[language=Python, basicstyle=\tiny]
% from scipy.sparse import spdiags
% import numpy as np

% def solveLaplaceEq(Nx, Ny, Tb, Tint):
%     e = np.ones(Nx*Ny)
%     diag_data = [e, e, -4*e, e, e]
%     diag_indices = [-Nx, -1, 0, 1, Nx]
%     A = spdiags(diag_data, diag_indices, Nx*Ny, Nx*Ny)
%     b = np.zeros(Nx*Ny)
    
%     # Set boundary conditions here (omitted for brevity)
%     A, b = setBoundaryConditions(A, b, Tb, Nx, Ny)

%     # Set a central node to Tint
%     ind = round(Nx * (Ny/2) + Nx/2)
%     A[ind, :] = 0
%     A[ind, ind] = 1
%     b[ind] = Tint

%     T = spsolve(A, b)
%     return T.reshape(Nx, Ny)
%     \end{lstlisting}
%   \end{frame}
  

%   \begin{frame}[fragile] 
%     \frametitle{Second example: curve\_fit}
%     Now let's make a goal function based on our model and the desired setpoint (the mean temperature in the domain):
    
%     \begin{lstlisting}[language=Python]
% def fitcrit_laplace(actuate_T, Nx, Ny, boundary_T, setpoint_T):
%     # Compute model:
%     T = solveLaplaceEq(Nx, Ny, boundary_T, actuate_T)
    
%     # Compute error (deviation of mean T with desired setpoint T)
%     err = np.mean(T) - setpoint_T
%     return err
%     \end{lstlisting}
%   \end{frame}  

%   \begin{frame}[fragile] 
%     \frametitle{Second example: curve\_fit}
%     \begin{itemize}
%       \item Set up system parameters
%       \item Run the fitting procedures
%       \item Compute and plot the final solution
%     \end{itemize}
    
%     \begin{lstlisting}
% from scipy.optimize import curve_fit
% import matplotlib.pyplot as plt
% from mpl_toolkits.mplot3d import Axes3D

% # Set up parameters
% Nx = 35; Ny = 35;
% Tb = [40, 10, 40, 10]     # Fixed boundary temperatures
% T0 = [0]                  # Initial guess 
% T_set = 20                # Setpoint

% # Run fitting
% params, params_covariance = curve_fit(lambda T, _: fitcrit_laplace(T, Nx, Ny, Tb, T_set), [0], [0], p0=T0)

% # Compute again and plot
% T_model = solveLaplaceEq(Nx, Ny, Tb, params[0])
% T_plot = T_model.reshape(Nx, Ny)       # Reshape x-vec to mat Nx, Ny
% x, y = np.meshgrid(range(Ny), range(Nx))  # Get position arrays

% fig = plt.figure()
% ax = fig.add_subplot(111, projection='3d')
% ax.plot_surface(x, y, T_plot)           # Surface plot
% plt.show()

% print(np.mean(T_model))
%     \end{lstlisting}
%   \end{frame}
  




% \section{Curve fitting}
% \subsection*{}
% \begin{frame}
%   \frametitle{Summary}
%   \begin{itemize}
%     \item We have seen how fit parameters of a model can fitted to a data set, using the linear least squares method.
%     \item We found out how to calculate the regression coefficients and how to perform a statistical analysis of the model using ANOVA.
%     \item We also postulated expressions for the confidence limits for the fit parameters and the predicted points.
%   \end{itemize}
% \end{frame}

\section{Optimization}
\subsection*{}
\againframe<2>{contents_opt}
% \frame{\partpage}
% \section{Introduction}
% \subsection*{General}
% \begin{frame}[label=contents]
%   \frametitle{Today's outline}
%   \mode<beamer>{
%     \only<1>{\tableofcontents}
%   }
%   \only<2>{\tableofcontents[currentsection,currentsubsection]}
% \end{frame}

% \begin{frame}
%   \frametitle{Overview}
%   \begin{itemize}
%     \item In this lecture we get introduced to constrained and unconstrained optimization.
%     \item We will use the simplex method to solve linear programming problems (LP)
% %     \item We will use the Lagrange multiplier method to solve nonlinear programming problems (NLPâ€™s) â€¢ And we will briefly discuss optimal control, using Pontryaginâ€™s principle.
% %     \item Lastly we will play a little with another optimization platform (AMPL)
%   \end{itemize}
% \end{frame}

\begin{frame}
  \frametitle{What is optimization?}
  \tikz{\node[emphblock, text width=\textwidth]{Optimization is minimization or maximization of an objective function (also called a performance index or goal function) that may be subject to certain constraints.}}
  \vskip1em
  \begin{itemize}
    \item $\min f(x)$: Goal function
    \item $g(x) = 0$: Equality constraints
    \item $h(x) \geq 0$: Inequality constraints
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Optimization Spectrum}
  % See http://www.tablesgenerator.com/#
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
Problem     & Method       & Solvers   \\ \hline
LP & \begin{tabular}[c]{@{}l@{}}Simplex method\\ Barrier methods\end{tabular} & \begin{tabular}[c]{@{}l@{}}Linprog\\ CPLEX (GAMS, AIMMS, AMPL, OPB)\end{tabular} \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}NLP\\ QP\end{tabular}} & {\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Lagrange multiplier method\\ Successive linear programming\\ Quadratic programming\end{tabular}}                         & {\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Fminsearch/fmincon (Matlab)\\ MINOS (GAMS, AMPL)\\ CONOPT (GAMS)\end{tabular}} \\
\begin{tabular}[c]{@{}l@{}}MIP\\ MILP\\ MINLP\\ MIQP\end{tabular}       & \begin{tabular}[c]{@{}l@{}}Branch and bound\\ Dynamic programming\\ Generalized Benders decomposition\\ Outer approximation method\\ Disjunctive programming\end{tabular} & \begin{tabular}[c]{@{}l@{}}Bintprog (Matlab)\\ DICOPT (GAMS)\\ BARON (GAMS)\end{tabular}                                        \\ \hline
\end{tabular}
}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Factors of concern}
  \begin{itemize}
    \item Continuity of the functions
    \item Convexity of the functions
    \item Global versus local optima
    \item Constrained versus unconstrained optima
  \end{itemize}
\end{frame}

\section{Linear programming}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}
  \frametitle{Linear programming}
  In linear programming the objective function and the constraints are linear functions.
  \vskip1em \pause
  
  \begin{columns}
    \column{0.5\textwidth}
    For example:\\ 
    \vskip1em      
      $\max z = f(x_1, x_2)=40x_1 + 88x_2$ \\
      s.t. (subject to) \\
      $2x_1 + 8x_2 \leq 60$ \\
      $5x_1 + 2x_2 \leq 60$ \\
      $x_1 \geq 0$ \\
      $x_2 \geq 0$
    \column{0.5\textwidth}
      \tikz{\node[emphblock, text width=\textwidth]{If the constraints are satisfied, but the objective function is not maximized/minimized we speak of a feasible solution. \\ \vskip1em
      If also the objective function is maximized/minimized, we speak of an optimal solution!}}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Plotting the constraints}
  \centering
  \input{tikzplots/optim1.tikz}
\end{frame}

\begin{frame}
  \frametitle{Plotting the constraints}
  \centering
  \input{tikzplots/optim2.tikz}
\end{frame}

\begin{frame}
  \frametitle{Normal form of an LP problem}
   \begin{columns}[T]
    \column{0.5\textwidth}
      $\max z = f(x_1, x_2)=40x_1 + 88x_2$ \\
      s.t. \\
      $2x_1 + 8x_2 \leq 60$ \\
      $5x_1 + 2x_2 \leq 60$ \\
      $x_1 \geq 0$ \\
      $x_2 \geq 0$
    \pause
    \column{0.5\textwidth}
      $\max z = f(x) = 40x_1 + 88x_2$ \\
      s.t. \\
      $2x_1 + 8x_2 + x_3 = 60$ \\
      $5x_1 + 2x_2 + x_4 = 60$ \\
      $x_i \geq 0 \qquad i \in 1,2,3,4$\\
      \vskip1em
      \tikz{\node[emphblock, text width=\columnwidth]{$x_3$ and $x_4$ are called slack variables, they are non auxiliary variables introduced for the purpose of converting inequalities in to equalities}}
  \end{columns}
  \end{frame}

  \begin{frame}
  \frametitle{The simplex method}
  We can formulate our earlier example to the normal form and consider it as the following augmented matrix with $T_0 = \left[z \quad x_1 \quad x_2 \quad x_3 \quad x_4 \quad b\right]$:
  \vskip1em
   \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   This matrix is called the (initial) simplex table. Each simplex table has two kinds of variables, the basic variables (columns having only one nonzero entry) and the nonbasic variables
\end{frame}

\begin{frame}
  \frametitle{The simplex method}
     \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   Every simplex table has a feasible solution. It can be obtained by setting the nonbasic variables to zero: $x_1 = 0$, $x_2 = 0$, $x_3 = 60/1$, $x_4 = 60/1$, $z = 0$.
\end{frame}

\begin{frame}
  \frametitle{The optimal solution?}
  \begin{itemize}
    \item The optimal solution is now obtained stepwise by pivoting in such way that z reaches a maximum. 
    \item The big question is, how to choose your pivot equation ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Step 1: Selection of the pivot column}
  Select as the column of the pivot, the first column with a negative entry in Row 1. In our example, thatâ€™s column 2 (-40)
     \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
\end{frame}

\begin{frame}
  \frametitle{Step 2: Selection of the pivot row}
  Divide the right sides by the corresponding column entries of the selected pivot column. In our example that is $60/2 = 30$ and $60/5 = 12$.
     \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   Take as the pivot equation the equation that gives the smallest quotient, so $60/5$.
\end{frame}

\begin{frame}
  \frametitle{Step 3: Elimination by row operations}
  \begin{itemize}
    \item Row 1 = Row 1 + 8 * Row 3
    \item Row 2 = Row 2 - 0.4 * Row 3
  \end{itemize}
     \[
T_1 = \begin{bmatrix}
1 & 0 & -72 & 0 & 8 & 480\\ 
0 & 0 & 7.2 & 1 & -0.4 & 36\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   The basic variables are now $x_1$, $x_3$ and the nonbasic variables are $x_2$, $x_4$. Setting the nonbasic variables to zero will give a new feasible solution: $x_1 = 60/5$, $x_2 = 0$, $x_3 = 36/1$, $x_4 = 0$, $z =480$.
\end{frame}

\begin{frame}
  \frametitle{The simplex method}
  \begin{itemize}
    \item We moved from z = 0 to z = 480. The reason for the increase is because we eliminated a negative term from the eqation, so: elimination should only be applied to negative entries in Row 1, but no others.
    \item Although we found a feasible solution, we did not find the optimal solution yet (the entry of -72 in our simplex table) $\longrightarrow$ repeat step 1 to 3. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The simplex method}
  Another iteration is required:
  \begin{itemize}
    \item Step 1: Select column 3
    \item Step 2: $36/7.2 = 5$ and $60/2 = 30$ $\longrightarrow$ select 7.2 as the pivot
    \item Elimination by row operations:
    \begin{itemize}
      \item Row 1 = Row 1 + 10*Row 2
      \item Row 3 = Row 3 - (2/7.2)*Row 2
    \end{itemize}
       \[
T_2 = \begin{bmatrix}
1 & 0 & 0 & 10 & 4 & 840\\ 
0 & 0 & 7.2 & 1 & -0.4 & 36\\
0 & 5 & 0 & -1/36 & 1/0.9 &50 
\end{bmatrix}
   \]
    \item The basic feasible solution: $x_1 = 50/5$, $x_2 = 36/7.2$, $x_3 = 0$, $x_4 = 0$, $z=840$ (no more negative entries: so this solution is also the optimal solution)
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Using Python for LP problems}
  \begin{columns}
    \column{0.5\textwidth}
    We are going to solve the following LP problem:\\
    \[
    \min f(x) = -5x_1 -4x_2 - 6x_3 
    \] 
    s.t. \\
    \[
    \begin{array}{l}
    x_1 - x_2 + x_3 \leq 20 \\
    3x_1 + 2x_2 + 4x_3 \leq 42 \\
    3x_1 + 2x_2 \leq 30 
    \end{array}
    \]
    \[
    \begin{array}{l}
    x_1 \geq 0 \\
    x_2 \geq 0 \\
    x_3 \geq 0
    \end{array}
    \]
    \column{0.5\textwidth}
    Using the function \lstinline|linprog| from \lstinline|scipy.optimize|:
    \begin{lstlisting}
from scipy.optimize import linprog

c = [-5, -4, -6]
A = [[1, -1, 1], [3, 2, 4], [3, 2, 0]]
b = [20, 42, 30]
bounds = [(0, None), (0, None), (0, None)]

res = linprog(c, A_ub=A, b_ub=b, bounds=bounds)
    \end{lstlisting}
Gives (after accessing appropriate attributes of the result object):
    \begin{lstlisting}
x = res.x
fun = res.fun
slack = res.slack
success = res.success
    \end{lstlisting}
  \end{columns}
\end{frame}


% \section{Non-linear programming}
% \begin{frame}
%   \frametitle{Non-linear programming}
%   \begin{itemize}
%     \item In nonlinear programming the objective function and the constraints are nonlinear functions. For example: \vskip1em
%     $\min f(x) = -5x_1^2 + 3x_2^2$ \\
%     s.t. \\
%     $g(x) = 2x_1 + x_2 - 5 = 0$ 
%     \vskip1em \pause
%     \item Using the Lagrange multiplier method, a Lagrangian function can be defined as:
%     \[
%       L(x,v) = f(x) + vg(x)
%     \] \pause
%   \item The optimum is found by differentiation of $L$ with respect to $x$ and $v$ and set the equations to zero:
%   \[
%     \frac{\partial L}{\partial x} = \frac{\partial f}{\partial x} + v\frac{\partial g}{\partial x} = 0, \, g(x) = 0
%   \]
%   \end{itemize}
% \end{frame}
% 
% \begin{frame}
%   \frametitle{Lagrange multiplier method}
%    $\min f(x) = -5x_1^2 + 3x_2^2$ \\
%     s.t. \\
%     $g(x) = 2x_1 + x_2 - 5 = 0$ \\
%     \vskip1em \pause
%     \[
%       L = 5x_1^2 + 3x_2^2 + v(2x_1 + x_2 - 5)
%     \]
%     \pause
%     \[ 
% \left.\begin{matrix}
% \frac{\partial L}{\partial x_1} = 10x_1 + 2v = 0 \\ 
% \frac{\partial L}{\partial x_2} = 6x_2 + v = 0  \\ 
% \frac{\partial L}{\partial v} = g(x) = 2x_1 + x_2 - 5 = 0
% \end{matrix}\right\} v = \frac{150}{17}, x_1=\frac{30}{17}, x_2=\frac{25}{17}
%     \]
% \end{frame}
% 
% \begin{frame}
%   \frametitle{LMM for non-linear problems with inequality constraints}
%    Consider a problem of the following form:
%    $\min f(x)$ \\
%     s.t. \\
%     $h_j(x) = 0 \quad \left{j=1,\ldots,m\right}$
%     $g_i(x) \geq 0 \quad \left{i=m+1,\ldots,p\right}$ \\
%     \vskip1em \pause
%     The Lagrangian function is defined as:
%     \[
%       L(x,u,v) = f(x) + \sum_{j=1}^{m} v_j h_j(x)+\sum_{j=m+1}^{p}u_j g_j(x)
%     \]
%     The \emph{Karush-Kuhn-Tucker} conditions state that the optimal solution is found 
% \end{frame}
\section{Summary}
\subsection*{}
\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
    \item Curve fitting: Manual procedures for polynomial fitting in Python
    \item Curve fitting: Python's SciPy library for curve fitting
    \item Curve fitting: Python's non-linear least-squares solver \lstinline$least\_squares$
    \item Curve fitting: Python's non-linear least-squares solver \lstinline$curve\_fit$
    \item Optimization: An introduction to the Simplex method in Python
    \item Optimization: Use of the \lstinline$linprog$ function in the SciPy library
  \end{itemize}
\end{frame}
