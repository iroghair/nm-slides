\part{Regression and optimization}
\section{Introduction}
\subsection*{}
\begin{frame}[label=contents_opt]
  \frametitle{Today's outline}
  \mode<beamer>{
    \only<1>{\tableofcontents}
  }
  \only<2>{\tableofcontents[currentsection,currentsubsection]}
\end{frame}

\frame{
  \frametitle{Overview}
  \vfill
  \begin{itemize}
  \item We are going to fit measurements to models today
  \item You will also learn what $R^2$ actually means
  \item We get introduced to constrained and unconstrained optimization.
  \item We will use the simplex method to solve linear programming problems (LP)
  \end{itemize}
  \vfill
}

\section{Curve fitting}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}[fragile,label={slidedatacreate}]{Let's do an 'experiment' to gather data}
    \begin{lstlisting}[basicstyle=\scriptsize]
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Generate linear space of control points
N = 100                   # Number of data points
x = np.linspace(0, 1, N)  # Points (independent variable)
A = [1, 1/3, 1.5, 3.5]    # Coefficients of polynomial

# Generate 'measurement values' with errors following a normal distribution
# Initialize randomizer
pd = norm(loc=0, scale=0.5)
# Add scatter data to the polynomial
y = A[3]*x**3 + A[2]*x**2 + A[1]*x + A[0] + pd.rvs(size=N)

# Plot the generated data
plt.plot(x, y, 'x')
plt.show()      
    \end{lstlisting}
\end{frame}

\frame{
  \frametitle{Fitting models to data}
  \vfill
  \centering
  \input{tikzplots/scatter1.tikz}
  \vfill
}

\frame{
  \frametitle{How to fit a model to the data?}
  We would like to fit the following model to the data:
  \[
    \hat{y} = a_1 + a_2x + a_3x^2 + a_4 x^3
  \]
  \vfill \pause
  First step: If we have $N$ data points, we could write the model as the product of a matrix and a vector:
  \begin{columns}\
    \column{0.6\textwidth}
      \[
      \begin{bmatrix}
      \hat{y}_1 \\
      \hat{y}_2 \\
      \hat{y}_3 \\
      \vdots \\
      \hat{y}_N
    \end{bmatrix}
    =
    \left. \begin{bmatrix}
      1 & x_1 & x_1^2 & x_1^3 \\
      1 & x_2 & x_2^2 & x_2^3 \\
      1 & x_3 & x_3^2 & x_3^3 \\
      \vdots  & \vdots  & \vdots  \\
      1 & x_N & x_N^2 & x_N^3
    \end{bmatrix}    
    \begin{bmatrix}
      a_1 \\
      a_2 \\
      a_3 \\
      a_4
    \end{bmatrix} \right.
      \]
    \column{0.4\textwidth}
    \tikz{\node[emphblock,text width=\columnwidth]{
    \begin{center}
    $
      \hat{y} = Xa
    $
    \end{center}
    $X$ is called the design matrix and $a$ are the fit parameters.
    }}
  \end{columns}
  \vfill
}

\frame{
  \frametitle{Residuals}
  Second step: work out the residuals for each data point:
  \[
    d_i = \left(y_i - \hat{y}_i\right)
  \]
  \pause
  Third step: work out the sum of squares of the residuals:
  \[
    \text{SSE} = \sum_i d_i^2 = \sum_i  \left(y_i - \hat{y}_i\right)^2
  \]
  This can be written using the dot-product operation:
    \[
    \text{SSE} = \sum_i d_i^2 = d \cdot d = d^T \cdot d = \left(y_i - \hat{y}_i\right)^T \cdot \left(y_i - \hat{y}_i\right)
  \]
}

\frame{
  \frametitle{Minimizing the sum of squares}
  Choose the parameter vector such that the sum of squares of the residuals is minimized; the partial derivative with respect to each parameter should be set to zero:
  \[
    \frac{\partial}{\partial a_i} \left[ \left(y - \left(Xa\right)^T\right)\left(y - Xa\right) \right]
  \]
  \pause
  In Matlab, we can solve our linear system $\hat{Y} = Xa$ simply by running \lstinline$a = np.linalg.solve(X,y)$.
  \pause
  \begin{itemize}
    \item If there are more data points ($N>4$), we can write an analogue, but maybe a consistent solution does not exist (the system is over specified).
    \item However, Python will find values for the vector a which minimize $\norm{y-aX}^2$ , so i.e. a least squares fit.
  \end{itemize}
}

% \frame{ 
%   \frametitle{Using Matlab's linear solver for fitting}
%   \begin{itemize}
%     \item If there are more data points ($N>4$), we can write an analogue, but maybe a consistent solution does not exist (the system is over specified).
%     \item However, matlab will find values for the vector a which minimize $\norm{y-aX}^2$ , so i.e. a least squares fit.
%   \end{itemize}
% }

\begin{frame}[fragile] 
  \frametitle{Fitting our problem: Least squares solver}
  As a follow-up of the script provided in slide~\ref{slidedatacreate}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \begin{lstlisting}
import numpy as np

# Assuming x and y are defined previously
N = len(x)
X = np.zeros((N, 4))
X[:, 0] = 1
X[:, 1] = x
X[:, 2] = x**2
X[:, 3] = x**3
A = np.linalg.lstsq(X, y, rcond=None)[0]      

# y_fit = X@A 
    \end{lstlisting}
    \column{0.5\textwidth}
      \input{tikzplots/scatter-model.tikz}
  \end{columns}
\end{frame}


\begin{frame}[fragile] 
  \frametitle{Fitting our problem: Excel}
    \begin{itemize}
      \item Create a column with the independent and dependent data series ($x$ and $y$)
      \item Create a column that computes $\hat{y}$, keeping the coefficients as separate cells
      \item Compute the sum of squares of the residuals (another column, sum the results)
      \item Use the solver to minimize this sum, modifying the coefficient cells
      \item Note: regression in Excel + display equation is dangerous if you choose the 'line' plot (use scatter if you can)
    \end{itemize}
\end{frame}

\begin{frame}[fragile] 
  \frametitle{How good is the model?}
  \input{tikzplots/residuals.tikz}

  \begin{itemize}
    \item For a model to make sense the data points should be scattered randomly around the model predictions, the mean of the residuals $d$ should be zero: $d_i = \left( y_i - \hat{y}_i\right)$
    \item It’s always good to check if the residuals are not correlated with the measured values, if that is the case, it can indicate that your model is wrong.
  \end{itemize}
\end{frame}

\section{Regression}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}[fragile] 
  \frametitle{Regression coefficients}
    \begin{itemize}
    \item Variance measured in the data (y) is:
    \[
      \sigma^2_y = \frac{1}{N} \sum_i \left (y_i - \overline{y}\right)^2
    \]
    \item Variance of the residuals is:
    \[
      \sigma^2_\text{error} = \frac{1}{N} \sum_i \left (d_i\right)^2
    \]
        \item Variance in the model is:
    \[
      \sigma^2_\text{model} = \frac{1}{N} \sum_i \left (\hat{y}_i - \overline{\hat{y}}\right)^2
    \]
  \end{itemize}
\end{frame}

\begin{frame}[fragile] 
  \frametitle{Regression coefficients}
  Given that the error is uncorrelated we can state that:
    \[
      \sigma^2_y = \sigma^2_\text{error}+\sigma^2_\text{model}
    \]
    \[
      R^2 = \frac{\sigma^2_\text{model}}{\sigma^2_y} = 1 - \frac{\sigma^2_\text{error}}{\sigma^2_y}
    \]
    \[
      R^2 = 1 - \frac{\text{SSE}}{\text{SST}}
    \]
    \begin{itemize}
      \item SSE: Sum of errors (residuals) squared (difference between data and model)
      \item SST: Total sum of squares (variance of the data)
      \item SSR: Sum of squares (model)
   \end{itemize}
\end{frame}

% \begin{frame}[fragile] 
%   \frametitle{Regression coefficients}
%     \begin{itemize}
%       \item An uncorrellated error ($\overline{d} \rightarrow 0$) means that SSE, SST and SSR will have $\chi^2$ -distributions and the ratios will have an F-distribution. If SSR/SSE is large, the model is good!
%       \item There is a chance that the model is rubbish, but that SSR/SSE will yield a good value, Analysis of Variance (ANOVA) will be a good tool to calculate the probability of such a thing happening!
%    \end{itemize}
% \end{frame}

\begin{frame}[fragile] 
  \frametitle{Back to the example}
  \begin{columns}
    \column{0.4\textwidth}
      \begin{center}
        The statistics: \\ \vskip1em
        % \input{tikzplots/table.tex}
        \begin{tabular}{lc}
          & Value \\ 
         \hline$N$   & 100 \\ 
         SSE   & 32.042 \\ 
         SST   & 896.907 \\ 
         SSR   & 928.950 \\ 
         $R^2$ & 0.964 \\ 
         \hline\end{tabular}
      \end{center}
    \column{0.6\textwidth}
      \input{tikzplots/scatter-model.tikz}
  \end{columns}
\end{frame}

\section{Fitting numerical models}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}[fragile]
  \frametitle{Curve fitting from command line: scipy.optimize.curve\_fit}
  Python offers various non-linear parameter and curve fitting tools that can be run from the command line. The function \lstinline$curve_fit$ from the \lstinline$scipy.optimize$ module allows to fit a model to a given dataset. Again, based on the data generated in slide~\ref{slidedatacreate}:
    \begin{lstlisting}[language=Python,basicstyle=\tiny]
from scipy.optimize import curve_fit

# Initial guess of coefficients
a0 = [1, 2, 1, 3]

# Define the model function
def curve_fit_model(xdata, a1, a2, a3, a4):
    return a1*xdata**3 + a2*xdata**2 + a3*xdata + a4

# Perform fitting, store resulting coeffs in a_fit
a_fit, _ = curve_fit(curve_fit_model, x, y, p0=a0)

# Run the model once more, with fitted coefficients
y_model = curve_fit_model(x, *a_fit)

plt.plot(x, y_model, '-r')
plt.show()
    \end{lstlisting}
    In the Python translation, the model function takes individual parameters separately rather than as an array, and we use the unpacking syntax (\lstinline$*$) to pass the fitted parameters when generating the \lstinline$y_model$ data.
  \end{frame}
  

  \begin{frame}[fragile] 
    \frametitle{Dynamic fitting of non-linear equations}
    You may encounter situations where the model data is slightly more complicated to obtain (e.g., a numerical model based on ODEs where coefficients are unknown), or you want to perform fitting of multiple functions/coefficients, or just want to automate things via scripts. Python's Scipy library gives access to powerful functions such as \lstinline$least_squares$ and \lstinline$curve_fit$.
  \end{frame}
    
  \begin{frame}[fragile] 
    \frametitle{General use of scipy.optimize.least\_squares}
    \begin{lstlisting}[language=Python]
from scipy.optimize import least_squares

result = least_squares(fun, k0, bounds=(lb, ub), xtol=1.0E-6, max_nfev=1000)
    \end{lstlisting}
    \begin{itemize}
      \item \lstinline$fun$ is a function handle to the fit criterion (e.g., \lstinline$myFitCrit$). The fit criterion function \lstinline$myFitCrit$ should return the residuals vector, e.g., $d_i = \left(y_i - \hat{y}_i\right)$. Here, $y_i$ would again be the measurement data and $\hat{y}$ the solution computed by a model.
      \item \lstinline$k0$ is the initial guess for the fitting coefficient (or: array of initial guesses when fitting multiple coefficients).
      \item \lstinline$lb$ and \lstinline$ub$ are the lower and upper boundaries for \lstinline$k0$. These should both be the size of the \lstinline$k0$-array.
      \item Use arguments such as \lstinline$xtol$ and \lstinline$max_nfev$ for more fine-grained control on the fit procedure. 
    \end{itemize}  
  \end{frame}
  
  
  \begin{frame}[fragile] 
  \frametitle{General use of scipy.optimize.curve\_fit}
  \begin{lstlisting}[language=Python]
from scipy.optimize import curve_fit

popt, pcov = curve_fit(fun, xdata, ydata, p0=k0, bounds=(lb, ub))
  \end{lstlisting}
  \begin{itemize}
    \item \lstinline$fun$ is the model function that you want to fit to your data. It takes the independent variable as the first argument and the parameters to fit as separate remaining arguments.
    \item \lstinline$xdata$ and \lstinline$ydata$ are the data points that you are fitting the model function to.
    \item \lstinline$k0$ is the initial guess for the parameters to be fitted.
    \item \lstinline$lb$ and \lstinline$ub$ are the lower and upper bounds for the parameters, respectively.
    \item \lstinline$popt$ will contain the optimized parameters, and \lstinline$pcov$ will contain the covariance matrix, which can give you an idea of the uncertainties of the estimates.
  \end{itemize}  
  \end{frame}
    

  \begin{frame}[fragile] 
    \frametitle{Example use of scipy.optimize.curve\_fit}
    We have experimental data stored in a file, possibly in a .csv or .txt format, containing $T$ and $U$ data. We want to fit a model with coefficients $k_1$ and $k_2$ with the following structure:
    \[
      \frac{du}{dt} = -k_1 u + k_2
    \]
    \pause
    \begin{itemize}
      \item First, we define a function that describes our model:
      \begin{lstlisting}[language=Python,basicstyle=\scriptsize]
from scipy.integrate import odeint

def simpleode(u, t, k1, k2):
    dudt = -k1*u + k2
    return dudt
      \end{lstlisting}
      Note that we supply the coefficients \(k_1\) and \(k_2\) as arguments to the function.
      \pause
      \item We create a fit criterion function:
      \begin{lstlisting}[language=Python,basicstyle=\scriptsize]
def fitcrit(t, k1, k2):
    u0 = 1.0
    u = odeint(simpleode, u0, t, args=(k1, k2))
    return u.ravel()
      \end{lstlisting}
    \end{itemize}
    \end{frame}
    
    \begin{frame}[fragile] 
    \frametitle{Example use of scipy.optimize.curve\_fit}
    Now let's make a script that uses \lstinline$curve_fit$ to yield k-values fitted to our dataset:
    \pause
    \begin{lstlisting}[language=Python,basicstyle=\scriptsize]
from scipy.optimize import curve_fit
import numpy as np

# Load your data here (adjust as necessary)
T, U = np.loadtxt('./scripts/used_by_students/lecture_archive_regression_optim/tudataset1.txt', unpack=True, skiprows=1)

# Initial guesses for model parameters
k0 = [1.0, 1.0]

# Perform the curve fitting
params, params_covariance = curve_fit(fitcrit, T, U, p0=k0)

print('Fitted coefficients:', params)
    \end{lstlisting}
    Our fitted coefficients are stored in \lstinline|params|. The \lstinline|params_covariance| gives an estimate of the covariance of the estimated parameters, offering an insight into the uncertainty of the fit.
    \end{frame}      

\begin{frame}[fragile] 
  \frametitle{Example use of lsqnonlin}
  \centering\input{tikzplots/lsqnonlinfit.tikz}
\end{frame}

\begin{frame}[fragile] 
  \frametitle{Postprocessing of results}
  The data returned by \lstinline|curve_fit| can be used to obtain the 95\% confidence intervals for the fitted parameters. Recall the command:
  \begin{lstlisting}[language=Python]
params, params_covariance = curve_fit(fitcrit, T, U, p0=k0)
  \end{lstlisting}
  We can use the square root of the diagonal of the covariance matrix, multiplied by a factor from the t-distribution to get the confidence bounds:
  \begin{lstlisting}[language=Python,basicstyle=\scriptsize]
from scipy import stats
import numpy as np

alpha = 0.05 # 95% confidence interval = 100*(1-alpha)
n = len(U)    # number of data points
p = len(params) # number of parameters

dof = max(0, n - p) # number of degrees of freedom
# t value for the dof and confidence level
tval = stats.t.ppf(1.0-alpha/2., dof) 
sigma = np.sqrt(np.diag(params_covariance))
ci = sigma * tval

print('Confidence intervals:')
print('k1:', params[0] - ci[0], params[0] + ci[0])
print('k2:', params[1] - ci[1], params[1] + ci[1])
  \end{lstlisting}
  \end{frame}


  \begin{frame}[fragile]
    \frametitle{Second example: curve\_fit}
    The model doesn't have to be an ODE; it can be any model that is part of the fitting goal function. Let's consider an (adapted) version of the Laplace equation solver, where we can set a single, central node to a certain temperature. We will illustrate this using Python, where we define a function to solve the Laplace equation:
    
    \begin{lstlisting}[language=Python, basicstyle=\tiny]
from scipy.sparse import spdiags
import numpy as np

def solveLaplaceEq(Nx, Ny, Tb, Tint):
    e = np.ones(Nx*Ny)
    diag_data = [e, e, -4*e, e, e]
    diag_indices = [-Nx, -1, 0, 1, Nx]
    A = spdiags(diag_data, diag_indices, Nx*Ny, Nx*Ny)
    b = np.zeros(Nx*Ny)
    
    # Set boundary conditions here (omitted for brevity)
    A, b = setBoundaryConditions(A, b, Tb, Nx, Ny)

    # Set a central node to Tint
    ind = round(Nx * (Ny/2) + Nx/2)
    A[ind, :] = 0
    A[ind, ind] = 1
    b[ind] = Tint

    T = spsolve(A, b)
    return T.reshape(Nx, Ny)
    \end{lstlisting}
  \end{frame}
  

  \begin{frame}[fragile] 
    \frametitle{Second example: curve\_fit}
    Now let's make a goal function based on our model and the desired setpoint (the mean temperature in the domain):
    
    \begin{lstlisting}[language=Python]
def fitcrit_laplace(actuate_T, Nx, Ny, boundary_T, setpoint_T):
    # Compute model:
    T = solveLaplaceEq(Nx, Ny, boundary_T, actuate_T)
    
    # Compute error (deviation of mean T with desired setpoint T)
    err = np.mean(T) - setpoint_T
    return err
    \end{lstlisting}
  \end{frame}  

  \begin{frame}[fragile] 
    \frametitle{Second example: curve\_fit}
    \begin{itemize}
      \item Set up system parameters
      \item Run the fitting procedures
      \item Compute and plot the final solution
    \end{itemize}
    
    \begin{lstlisting}[language=Python,basicstyle=\tiny]
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Set up parameters
Nx = 35; Ny = 35;
Tb = [40, 10, 40, 10]     # Fixed boundary temperatures
T0 = [0]                  # Initial guess 
T_set = 20                # Setpoint

# Run fitting
params, params_covariance = curve_fit(lambda T, _: fitcrit_laplace(T, Nx, Ny, Tb, T_set), [0], [0], p0=T0)

# Compute again and plot
T_model = solveLaplaceEq(Nx, Ny, Tb, params[0])
T_plot = T_model.reshape(Nx, Ny)       # Reshape x-vec to mat Nx, Ny
x, y = np.meshgrid(range(Ny), range(Nx))  # Get position arrays

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(x, y, T_plot)           # Surface plot
plt.show()

print(np.mean(T_model))
    \end{lstlisting}
  \end{frame}
  




% \section{Curve fitting}
% \subsection*{}
% \begin{frame}
%   \frametitle{Summary}
%   \begin{itemize}
%     \item We have seen how fit parameters of a model can fitted to a data set, using the linear least squares method.
%     \item We found out how to calculate the regression coefficients and how to perform a statistical analysis of the model using ANOVA.
%     \item We also postulated expressions for the confidence limits for the fit parameters and the predicted points.
%   \end{itemize}
% \end{frame}

\section{Optimization}
\subsection*{}
\againframe<2>{contents_opt}
% \frame{\partpage}
% \section{Introduction}
% \subsection*{General}
% \begin{frame}[label=contents]
%   \frametitle{Today's outline}
%   \mode<beamer>{
%     \only<1>{\tableofcontents}
%   }
%   \only<2>{\tableofcontents[currentsection,currentsubsection]}
% \end{frame}

% \begin{frame}
%   \frametitle{Overview}
%   \begin{itemize}
%     \item In this lecture we get introduced to constrained and unconstrained optimization.
%     \item We will use the simplex method to solve linear programming problems (LP)
% %     \item We will use the Lagrange multiplier method to solve nonlinear programming problems (NLP’s) • And we will briefly discuss optimal control, using Pontryagin’s principle.
% %     \item Lastly we will play a little with another optimization platform (AMPL)
%   \end{itemize}
% \end{frame}

\begin{frame}
  \frametitle{What is optimization?}
  \tikz{\node[emphblock, text width=\textwidth]{Optimization is minimization or maximization of an objective function (also called a performance index or goal function) that may be subject to certain constraints.}}
  \vskip1em
  \begin{itemize}
    \item $\min f(x)$: Goal function
    \item $g(x) = 0$: Equality constraints
    \item $h(x) \geq 0$: Inequality constraints
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Optimization Spectrum}
  % See http://www.tablesgenerator.com/#
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\hline
\rowcolor[HTML]{C0C0C0} 
Problem                                                                 & Method                                                                                                                                                                    & Solvers                                                                                                                         \\ \hline
LP                                                                      & \begin{tabular}[c]{@{}l@{}}Simplex method\\ Barrier methods\end{tabular}                                                                                                  & \begin{tabular}[c]{@{}l@{}}Linprog (Matlab)\\ CPLEX (GAMS, AIMMS, AMPL, OPB)\end{tabular}                                       \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}NLP\\ QP\end{tabular}} & {\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Lagrange multiplier method\\ Successive linear programming\\ Quadratic programming\end{tabular}}                         & {\color[HTML]{000000} \begin{tabular}[c]{@{}l@{}}Fminsearch/fmincon (Matlab)\\ MINOS (GAMS, AMPL)\\ CONOPT (GAMS)\end{tabular}} \\
\begin{tabular}[c]{@{}l@{}}MIP\\ MILP\\ MINLP\\ MIQP\end{tabular}       & \begin{tabular}[c]{@{}l@{}}Branch and bound\\ Dynamic programming\\ Generalized Benders decomposition\\ Outer approximation method\\ Disjunctive programming\end{tabular} & \begin{tabular}[c]{@{}l@{}}Bintprog (Matlab)\\ DICOPT (GAMS)\\ BARON (GAMS)\end{tabular}                                        \\ \hline
\end{tabular}
}
\end{table}
\end{frame}

\begin{frame}
  \frametitle{Factors of concern}
  \begin{itemize}
    \item Continuity of the functions
    \item Convexity of the functions
    \item Global versus local optima
    \item Constrained versus unconstrained optima
  \end{itemize}
\end{frame}

\section{Linear programming}
\subsection*{}
\againframe<2>{contents_opt}
\begin{frame}
  \frametitle{Linear programming}
  In linear programming the objective function and the constraints are linear functions.
  \vskip1em \pause
  
  \begin{columns}
    \column{0.5\textwidth}
    For example:\\ 
    \vskip1em      
      $\max z = f(x_1, x_2)=40x_1 + 88x_2$ \\
      s.t. (subject to) \\
      $2x_1 + 8x_2 \leq 60$ \\
      $5x_1 + 2x_2 \leq 60$ \\
      $x_1 \geq 0$ \\
      $x_2 \geq 0$
    \column{0.5\textwidth}
      \tikz{\node[emphblock, text width=\textwidth]{If the constraints are satisfied, but the objective function is not maximized/minimized we speak of a feasible solution. \\ \vskip1em
      If also the objective function is maximized/minimized, we speak of an optimal solution!}}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Plotting the constraints}
  \centering
  \input{tikzplots/optim1.tikz}
\end{frame}

\begin{frame}
  \frametitle{Plotting the constraints}
  \centering
  \input{tikzplots/optim2.tikz}
\end{frame}

\begin{frame}
  \frametitle{Normal form of an LP problem}
   \begin{columns}[T]
    \column{0.5\textwidth}
      $\max z = f(x_1, x_2)=40x_1 + 88x_2$ \\
      s.t. \\
      $2x_1 + 8x_2 \leq 60$ \\
      $5x_1 + 2x_2 \leq 60$ \\
      $x_1 \geq 0$ \\
      $x_2 \geq 0$
    \pause
    \column{0.5\textwidth}
      $\max z = f(x) = 40x_1 + 88x_2$ \\
      s.t. \\
      $2x_1 + 8x_2 + x_3 = 60$ \\
      $5x_1 + 2x_2 + x_4 = 60$ \\
      $x_i \geq 0 \qquad i \in 1,2,3,4$\\
      \vskip1em
      \tikz{\node[emphblock, text width=\columnwidth]{$x_3$ and $x_4$ are called slack variables, they are non auxiliary variables introduced for the purpose of converting inequalities in to equalities}}
  \end{columns}
  \end{frame}

  \begin{frame}
  \frametitle{The simplex method}
  We can formulate our earlier example to the normal form and consider it as the following augmented matrix with $T_0 = \left[z \quad x_1 \quad x_2 \quad x_3 \quad x_4 \quad b\right]$:
  \vskip1em
   \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   This matrix is called the (initial) simplex table. Each simplex table has two kinds of variables, the basic variables (columns having only one nonzero entry) and the nonbasic variables
\end{frame}

\begin{frame}
  \frametitle{The simplex method}
     \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   Every simplex table has a feasible solution. It can be obtained by setting the nonbasic variables to zero: $x_1 = 0$, $x_2 = 0$, $x_3 = 60/1$, $x_4 = 60/1$, $z = 0$.
\end{frame}

\begin{frame}
  \frametitle{The optimal solution?}
  \begin{itemize}
    \item The optimal solution is now obtained stepwise by pivoting in such way that z reaches a maximum. 
    \item The big question is, how to choose your pivot equation ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Step 1: Selection of the pivot column}
  Select as the column of the pivot, the first column with a negative entry in Row 1. In our example, that’s column 2 (-40)
     \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
\end{frame}

\begin{frame}
  \frametitle{Step 2: Selection of the pivot row}
  Divide the right sides by the corresponding column entries of the selected pivot column. In our example that is $60/2 = 30$ and $60/5 = 12$.
     \[
T_0 = \begin{bmatrix}
1 & -40 & -88 & 0 & 0 & 0\\ 
0 & 2 & 8 & 1 & 0 & 60\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   Take as the pivot equation the equation that gives the smallest quotient, so $60/5$.
\end{frame}

\begin{frame}
  \frametitle{Step 3: Elimination by row operations}
  \begin{itemize}
    \item Row 1 = Row 1 + 8 * Row 3
    \item Row 2 = Row 2 - 0.4 * Row 3
  \end{itemize}
     \[
T_1 = \begin{bmatrix}
1 & 0 & -72 & 0 & 8 & 480\\ 
0 & 0 & 7.2 & 1 & -0.4 & 36\\
0 & 5 & 2 & 0 & 1 &60 
\end{bmatrix}
   \]
   The basic variables are now $x_1$, $x_3$ and the nonbasic variables are $x_2$, $x_4$. Setting the nonbasic variables to zero will give a new feasible solution: $x_1 = 60/5$, $x_2 = 0$, $x_3 = 36/1$, $x_4 = 0$, $z =480$.
\end{frame}

\begin{frame}
  \frametitle{The simplex method}
  \begin{itemize}
    \item We moved from z = 0 to z = 480. The reason for the increase is because we eliminated a negative term from the eqation, so: elimination should only be applied to negative entries in Row 1, but no others.
    \item Although we found a feasible solution, we did not find the optimal solution yet (the entry of -72 in our simplex table) $\longrightarrow$ repeat step 1 to 3. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The simplex method}
  Another iteration is required:
  \begin{itemize}
    \item Step 1: Select column 3
    \item Step 2: $36/7.2 = 5$ and $60/2 = 30$ $\longrightarrow$ select 7.2 as the pivot
    \item Elimination by row operations:
    \begin{itemize}
      \item Row 1 = Row 1 + 10*Row 2
      \item Row 3 = Row 3 - (2/7.2)*Row 2
    \end{itemize}
       \[
T_2 = \begin{bmatrix}
1 & 0 & 0 & 10 & 4 & 840\\ 
0 & 0 & 7.2 & 1 & -0.4 & 36\\
0 & 5 & 0 & -1/36 & 1/0.9 &50 
\end{bmatrix}
   \]
    \item The basic feasible solution: $x_1 = 50/5$, $x_2 = 36/7.2$, $x_3 = 0$, $x_4 = 0$, $z=840$ (no more negative entries: so this solution is also the optimal solution)
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Using Python for LP problems}
  \begin{columns}
    \column{0.5\textwidth}
    We are going to solve the following LP problem:\\
    \[
    \min f(x) = -5x_1 -4x_2 - 6x_3 
    \] 
    s.t. \\
    \[
    \begin{array}{l}
    x_1 - x_2 + x_3 \leq 20 \\
    3x_1 + 2x_2 + 4x_3 \leq 42 \\
    3x_1 + 2x_2 \leq 30 
    \end{array}
    \]
    \[
    \begin{array}{l}
    x_1 \geq 0 \\
    x_2 \geq 0 \\
    x_3 \geq 0
    \end{array}
    \]
    \column{0.5\textwidth}
    Using the function \texttt{linprog} from \texttt{scipy.optimize}:
    \begin{lstlisting}[language=Python, basicstyle=\scriptsize]
from scipy.optimize import linprog

c = [-5, -4, -6]
A = [[1, -1, 1], [3, 2, 4], [3, 2, 0]]
b = [20, 42, 30]
bounds = [(0, None), (0, None), (0, None)]

res = linprog(c, A_ub=A, b_ub=b, bounds=bounds)
    \end{lstlisting}
Gives (after accessing appropriate attributes of the result object):
    \begin{lstlisting}[language=Python, basicstyle=\scriptsize]
x = res.x
fun = res.fun
slack = res.slack
success = res.success
    \end{lstlisting}
  \end{columns}
\end{frame}


% \section{Non-linear programming}
% \begin{frame}
%   \frametitle{Non-linear programming}
%   \begin{itemize}
%     \item In nonlinear programming the objective function and the constraints are nonlinear functions. For example: \vskip1em
%     $\min f(x) = -5x_1^2 + 3x_2^2$ \\
%     s.t. \\
%     $g(x) = 2x_1 + x_2 - 5 = 0$ 
%     \vskip1em \pause
%     \item Using the Lagrange multiplier method, a Lagrangian function can be defined as:
%     \[
%       L(x,v) = f(x) + vg(x)
%     \] \pause
%   \item The optimum is found by differentiation of $L$ with respect to $x$ and $v$ and set the equations to zero:
%   \[
%     \frac{\partial L}{\partial x} = \frac{\partial f}{\partial x} + v\frac{\partial g}{\partial x} = 0, \, g(x) = 0
%   \]
%   \end{itemize}
% \end{frame}
% 
% \begin{frame}
%   \frametitle{Lagrange multiplier method}
%    $\min f(x) = -5x_1^2 + 3x_2^2$ \\
%     s.t. \\
%     $g(x) = 2x_1 + x_2 - 5 = 0$ \\
%     \vskip1em \pause
%     \[
%       L = 5x_1^2 + 3x_2^2 + v(2x_1 + x_2 - 5)
%     \]
%     \pause
%     \[ 
% \left.\begin{matrix}
% \frac{\partial L}{\partial x_1} = 10x_1 + 2v = 0 \\ 
% \frac{\partial L}{\partial x_2} = 6x_2 + v = 0  \\ 
% \frac{\partial L}{\partial v} = g(x) = 2x_1 + x_2 - 5 = 0
% \end{matrix}\right\} v = \frac{150}{17}, x_1=\frac{30}{17}, x_2=\frac{25}{17}
%     \]
% \end{frame}
% 
% \begin{frame}
%   \frametitle{LMM for non-linear problems with inequality constraints}
%    Consider a problem of the following form:
%    $\min f(x)$ \\
%     s.t. \\
%     $h_j(x) = 0 \quad \left{j=1,\ldots,m\right}$
%     $g_i(x) \geq 0 \quad \left{i=m+1,\ldots,p\right}$ \\
%     \vskip1em \pause
%     The Lagrangian function is defined as:
%     \[
%       L(x,u,v) = f(x) + \sum_{j=1}^{m} v_j h_j(x)+\sum_{j=m+1}^{p}u_j g_j(x)
%     \]
%     The \emph{Karush-Kuhn-Tucker} conditions state that the optimal solution is found 
% \end{frame}
\section{Summary}
\subsection*{}
\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
    \item Curve fitting: Manual procedures for polynomial fitting in Python
    \item Curve fitting: Python's SciPy library for curve fitting
    \item Curve fitting: Python's non-linear least-squares solver \lstinline$least\_squares$
    \item Curve fitting: Python's non-linear least-squares solver \lstinline$curve\_fit$
    \item Optimization: An introduction to the Simplex method in Python
    \item Optimization: Use of the \lstinline$linprog$ function in the SciPy library
  \end{itemize}
\end{frame}
